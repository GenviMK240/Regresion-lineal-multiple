---
title: "Regresión lineal multiple"
author: "Genoveva Serrano Fernández"
date: "2024-03-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##1.Cargue el fichero “spears.xls”
```{r echo=TRUE}
library(readxl)
spear <- read_excel("C:/Regresion lineal multiple/spear.xlsx")
View(spear)

spear<- as.data.frame(read_excel("C:/Regresion lineal multiple/spear.xlsx"))
  spear
```

##2.Realice un análisis exploratorio de las variables. Comente brevemente los resultados. (NI PUTA IDEA TT)
```{r echo=TRUE}

spear <- data.frame(
  id = 1:10,
 Maxle = c(18.936, 22.176, 22.104, 28.224, 18.720, 21.600, 23.472, 30.816, 30.456, 27.864),
  Socle = c(5.544, 6.408, 6.840, 7.848, 3.456, 2.664, 3.960, 4.032, 4.752, 6.624),
  Maxwi = c(25.416, 31.320, 28.800, 14.544, 29.160, 21.960, 33.624, 28.800, 33.048, 29.376),
  Upsoc = c(23.400, 26.856, 25.344, 28.224, 20.880, 21.600, 28.440, 30.816, 30.456, 32.688),
  Losoc = c(4.608, 6.408, 6.840, 7.848, 3.456, 2.664, 3.960, 4.032, 4.752, 6.624),
  Maxwit = c(45.000, 49.320, 51.120, 53.280, 42.480, 37.440, 53.136, 61.560, 53.784, 55.080),
  Weight = c(4.752, 9.576, 6.912, 7.776, 3.528, 2.376, 8.424, 9.072, 11.664, 7.344))

1.PRUEBA DE NORMALIDAD SHAPIRO-WILK
shapiro.test(spear$Maxle)
shapiro.test(spear$Socle)
shapiro.test(spear$Maxwi)
shapiro.test(spear$Upsoc)
shapiro.test(spear$Losoc)
shapiro.test(spear$Maxwit)
shapiro.test(spear$Weight)

Si el valor p es mayor que el nivel de significancia (generalmente 0.05), no hay evidencia suficiente para rechazar la hipótesis nula, lo que sugiere que los datos pueden seguir una distribución normal. De lo contrario, si el valor p es menor que el nivel de significancia, como es el caso de la variable Maxle unicamente, se rechaza la hipótesis nula, lo que indica que los datos no siguen una distribución normal.

2.GRAFICOS DE DISTRIBUCION

hist(spear$Maxle, main = "Histograma de Maxle", xlab = "Maxle")
hist(spear$Socle, main = "Histograma de Socle", xlab = "Socle")
hist(spear$Maxwi, main = "Histograma de Maxwi", xlab = "Maxwi")
hist(spear$Maxwit, main = "Histograma de Maxwit", xlab = "Maxwit")
hist(spear$Upsoc, main = "Histograma de Upsoc", xlab = "Upsoc")
hist(spear$Losoc, main = "Histograma de Losoc", xlab = "Losoc")
hist(spear$Weight, main = "Histograma de Weight", xlab = "Weight")

En los graficos no se llega a visualizar si poseen un patron o no, por ende, por este metodo se hace complicado establecer la distribucion de las variables.

3.MATRIZ DE CORRELACION

correlacion <- cor(spear)

print(correlacion)

En este caso, las variables con valor cercano a 1 indica una correlación positiva fuerte, mientras que un valor cercano a -1 indica una correlación negativa. 

library(corrplot)
corrplot(correlacion, method = "circle")

#Preguntas para el estudio: ¿Qué tipo de variación existe dentro de cada una de mis variables?¿Qué tipo de covariación ocurre entre mis diferentes variables?


#Distribucion en diagrama de cajas

spear_long <- gather(spear, key = "Variable", value = "Valor", -id)

ggplot(spear_long, aes(x = Variable, y = Valor)) +
  geom_boxplot(fill = "lightblue", color = "blue") +
  labs(x = "Variable", y = "Valor", title = "Diagrama de Cajas") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#Grafico de barras
ggplot(spear_long, aes(x = Variable, y = Valor, fill = Variable)) +
  geom_bar(stat = "identity") +
  labs(x = "Variable", y = "Valor", title = "Gráfico de Barras") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#Histogramas de variable
  Maxle <- "Maxle"
  Socle <- "Socle"
  Maxwi <- "Maxwi"
  Upsoc <- "Upsoc"
  Losoc <- "Losoc"
  Maxwit <- "Maxwit"
  Weight <- "Weight"
  
ggplot(spear, aes(x = !!as.symbol("Maxle"))) +
  geom_histogram(binwidth = 2, fill = "lightblue", color = "black") +
  labs(x = "Maxle", y = "Frecuencia", title = "Histograma de la Variable") +
  theme_minimal()

ggplot(spear, aes(x = !!as.symbol("Socle"))) +
  geom_histogram(binwidth = 2, fill = "blue", color = "black") +
  labs(x = "Socle", y = "Frecuencia", title = "Histograma de la Variable") +
  theme_minimal()

ggplot(spear, aes(x = !!as.symbol("Maxwi"))) +
  geom_histogram(binwidth = 2, fill = "pink", color = "black") +
  labs(x = "Maxwi", y = "Frecuencia", title = "Histograma de la Variable") +
  theme_minimal()

ggplot(spear, aes(x = !!as.symbol("Upsoc"))) +
  geom_histogram(binwidth = 2, fill = "yellow", color = "black") +
  labs(x = "Upsoc", y = "Frecuencia", title = "Histograma de la Variable") +
  theme_minimal()

ggplot(spear, aes(x = !!as.symbol("Losoc"))) +
  geom_histogram(binwidth = 2, fill = "orange", color = "black") +
  labs(x = "Losoc", y = "Frecuencia", title = "Histograma de la Variable") +
  theme_minimal()

ggplot(spear, aes(x = !!as.symbol("Maxwit"))) +
  geom_histogram(binwidth = 2, fill = "red", color = "black") +
  labs(x = "Maxwit", y = "Frecuencia", title = "Histograma de la Variable") +
  theme_minimal()

ggplot(spear, aes(x = !!as.symbol("Weight"))) +
  geom_histogram(binwidth = 2, fill = "purple", color = "black") +
  labs(x = "Weight", y = "Frecuencia", title = "Histograma de la Variable") +
  theme_minimal()

```

##3.Verifique los supuestos que han de cumplir las variables implicadas en un análisis de la regresión lineal múltiple. Comente los resultados. 
```{r echo=TRUE}
1.Prueba de linealidad: Grafico de dispersion

plot(spear$Socle, spear$Maxle, xlab = "Socle", ylab = "Maxle", main = "LINEALIDAD")
abline(lm(Maxle ~ Socle, data = spear), col = "red")  

plot(spear$Socle, spear$Maxwi, xlab = "Socle", ylab = "Maxwi", main = "LINEALIDAD")
abline(lm(Maxle ~ Socle, data = spear), col = "purple")  

plot(spear$Socle, spear$Upsoc, xlab = "Socle", ylab = "Upsoc", main = "LINEALIDAD")
abline(lm(Maxle ~ Socle, data = spear), col = "green")  

plot(spear$Socle, spear$Losoc, xlab = "Socle", ylab = "Losoc", main = "LINEALIDAD")
abline(lm(Maxle ~ Socle, data = spear), col = "black") 

plot(spear$Socle, spear$Maxwit, xlab = "Socle", ylab = "Maxwit", main = "LINEALIDAD")
abline(lm(Maxle ~ Socle, data = spear), col = "orange")  

plot(spear$Socle, spear$Weight, xlab = "Socle", ylab = "Weight", main = "LINEALIDAD")
abline(lm(Maxle ~ Socle, data = spear), col = "pink")  

2.Prueba de normalidad de los residuos: Ajustar el modelo de regresión lineal múltiple
modelo <- lm(Maxle ~ Socle + Maxwi + Upsoc + Losoc + Maxwit + Weight, data = spear)

3.Obtiene los residuos del modelo
residuos <- resid(modelo)

4.Prueba de Shapiro-Wilk para normalidad de los residuos
shapiro.test(residuos)

5.Gráfico Q-Q de los residuos
qqnorm(residuos)
qqline(residuos)

6.Prueba de homocedasticidad:Gráfico de residuos estandarizados contra valores ajustados
plot(fitted(modelo), rstandard(modelo), xlab = "Valores ajustados", ylab = "Residuos estandarizados", main = "Residuos vs Valores ajustados")

Prueba de Breusch-Pagan para homocedasticidad
library(lmtest)
bptest(modelo)

7.Prueba de independencia de los residuos: Gráfico de autocorrelación de los residuos
acf(residuos)

Prueba de Durbin-Watson para independencia de los residuos
durbinWatsonTest(modelo)


```
El modelo cumple la regresion lineal, sin embargo solamente no cumple con la linealidad ya que el patron de las variables es bastante dispersos, sin embargo, el patron de residuos es correcto ya que adopta la forma de una linea diagonal y su p-value es mayor a 0,05. Igualmente, presenta homocedasticidad al ser el resultado del test un p-value mayor a 0,05. En la Autocorrelacion D-W sugiere la presencia de autocorrelación positiva aunque la prueba no es significativa debido a que el p-valor asociado no es menor que el nivel de significancia usual de 0.05.

##4.Visualice la distribución de frecuencia de las variables.
```{r echo=TRUE}
frecuencia <- gather(spear, key = "Variable", value = "Valor", -id)

ggplot(frecuencia, aes(x = Valor)) +
  geom_histogram(binwidth = 2, fill = "cyan", color = "black") +
  facet_wrap(~ Variable, scales = "free") +
  labs(x = "Valor", y = "Frecuencia", title = "Distribución de Frecuencia de las Variables") +
  theme_minimal()
```

##5.¿Qué valores ha de registrar la prueba de Shapiro-Wilk para indicar que se trata de una variable con distribución normal?
Los valores que han de registrar en esta prueba en el caso de una variable con distribucion normal deben ser mayores que el nivel de significancia, normalmente 0.05.

##6.¿Como verificaría el supuesto de linealidad?
Por ejemplo, para visualizar la relacion entre las variables podriamos elaborar un grafico de dispersion, pudiendo incluso analizar y graficar posteriormente los valores de residuos. Ambos diagramos nos permitirian observar si los datos de la tabla cumplen una distribución lineal o no.
```{r echo= TRUE}
En primer lugar, antes de examinar la relacion entre las variables, deberemos de ajustar el modelo de regresion lineal. 

modelo <- lm(Maxle ~ Socle + Maxwi + Upsoc + Losoc + Maxwit + Weight, data = spear)

Luego obtenemos los residuos

residuos <- residuals(modelo)

A continuacion,realizamos diagramas de dispersion de comparacion entre variables_continuas

  Maxle <- "Maxle"
  Socle <- "Socle"
  Maxwi <- "Maxwi"
  Upsoc <- "Upsoc"
  Losoc <- "Losoc"
  Maxwit <- "Maxwit"
  Weight <- "Weight"
       
par(mfrow = c(2, 3))  
for (var in names(spear)[-1]) {  
  plot(spear[[var]], spear$Maxle, main = paste("Diagrama de dispersión de Maxle", var),
       xlab = var, ylab = "Maxle", col = "pink")
}

for (var in names(spear)[-1]) {  
  plot(spear[[var]], spear$Socle, main = paste("Diagrama de dispersión de Socle", var),
       xlab = var, ylab = "Socle", col = "blue")
}

for (var in names(spear)[-1]) {  
  plot(spear[[var]], spear$Maxwi, main = paste("Diagrama de dispersión de Maxwi", var),
       xlab = var, ylab = "Maxwi", col = "darkgreen")
}

for (var in names(spear)[-1]) {  
  plot(spear[[var]], spear$Upsoc, main = paste("Diagrama de dispersión de Upsoc", var),
       xlab = var, ylab = "Upsoc", col = "orange")
}

for (var in names(spear)[-1]) {  
  plot(spear[[var]], spear$Losoc, main = paste("Diagrama de dispersión de Losoc", var),
       xlab = var, ylab = "Losoc", col = "purple")
}

for (var in names(spear)[-1]) {  
  plot(spear[[var]], spear$Maxwit, main = paste("Diagrama de dispersión de Maxwit", var),
       xlab = var, ylab = "Maxwit", col = "brown")
}

for (var in names(spear)[-1]) {  
  plot(spear[[var]], spear$Weight, main = paste("Diagrama de dispersión de Weight", var),
       xlab = var, ylab = "Weight", col = "red")
}

Ahora se grafican los residuos y los valores ajustados de la variables_continuas

plot(fitted(modelo), residuos, main = "Residuos vs. Valores Ajustados",
     xlab = "Valores Ajustados", ylab = "Residuos", col = "magenta")
abline(h = 0, col = "black")
              
              
```
La distribucion de todas las variables suguiere que la relación entre la variable dependiente y las variables independientes es lineal y que los residuos tienen una distribución constante y homocedástica.

##7.Visualice la matriz de dispersión entre variables para averiguar la intensidad de la relación entre dos variables.
```{r echo=TRUE}
Para visualizar la matriz haremos uso de la funcion pairs() junto con la librería psych. Generará una matriz de dispersión que muestra la relación entre todas las variables.

library(psych)
pairs(spear[-1])
```
##8. y visualice las variables que mayor correlación poseen en una nueva matriz de dispersión.
```{r echo=TRUE}
Para ello en primer lugar debo hacer la matriz de correlacion:
  
 matriz_correlacion <- cor(spear[, -1])

Luego tengo que encontrar las variables con mayor correlacion:
  
mayor_correlacion <- which(matriz_correlacion == max(matriz_correlacion[upper.tri(matriz_correlacion)]), arr.ind = TRUE)

Extraigo las variables con mayor correlacion:

Variable1 <- rownames(matriz_correlacion)[mayor_correlacion[1, 1]]
Variable2 <- colnames(matriz_correlacion)[mayor_correlacion[1, 2]]

Y para visualizar los resultados, creo una matriz de dispersion:
  
pairs(spear[c(Variable1, Variable2)])
```
##9.Realice la prueba de correlación lineal de Pearson. Comente los valores arrojados por los distintos coeficiente que arroja esta prueba.
```{r echo=TRUE}

Pearson <- cor.test(spear$Maxle, spear$Socle, method = "pearson")
print(Pearson)
```
##10.¿Qué nos indica el p-value?
Es el indice que indica si la correlación entre las variables podría ser el resultado del azar. Al ser la prueba de Pearson realizada sobre un nivel de significancia del 95%, el p value indicaría dentro de ese 5% restante si el resultado es azar, siempre y cuando el pvalue sea mayor a 0.05, como es nuestro caso.

##11.Defina brevemente “nivel de significancia”.
Es un valor predefinido que se asigna en pruebas de hipótesis estadísticas para corroborar la validez de una afirmación. Es como tal la probabilidad de cometer un error de tipo I al rechazar la hipótesis nula cuando es verdadera.

##12.¿Cuáles son la hipótesis planteadas en esta prueba (Pearson)?
Las hipotesis planteadas en esta prueba son dos: 

Hipótesis nula (H0): No hay una correlación lineal significativa entre las variables Maxle y Socle.

Hipótesis alternativa (H1): Existe una correlación lineal significativa entre las variables Maxle y Socle.

##13.¿Qué quiere decir “Subset selection”? ¿Qué importancia tiene en la regresión lineal múltiple?

El "Subset selection" en la regresión múltiple es usado para seleccionar un subconjunto de variables independientes que se utilizarán en el modelo de regresión. Consiste principalmente en evaluar todas las combinaciones posibles de las variables independientes y  que proporcionen elesoger las que mejor ajuste al modelo en función de algún criterio sleccionado previamente.

La importancia de este radica en que es lo que permite identificar las variables más relevantes que contribuyen  a explicar la variabilidad en la variable dependiente. Basicamente, al seleccional personalmente los subconjuntos de variables independientes, lo que estamos haciendo es simplificar el modelo evitando la inclusion de variables redundantes o no utiles, mejorando y facilitando así la interpretabilidad del modelo. 

##14.Dígame una estrategia eficaz en el contexto de “Subset Selection”.
Las estrategias eficaces deben usar criterios de selección adecuados para evaluar y comparar los diferentes subconjuntos. Uno de los ejemplo de criterio más común es el criterio de información de Akaike (AIC). Este criterio se lleva a cabo siguiendo estos pasos:

1.Generar un modelo con todos los posibles subconjuntos. Para ellos ya tenemos elaborado nuestro dataframe:
```{r echo=TRUE}
spear <- data.frame(
  id = 1:10,
 Maxle = c(18.936, 22.176, 22.104, 28.224, 18.720, 21.600, 23.472, 30.816, 30.456, 27.864),
  Socle = c(5.544, 6.408, 6.840, 7.848, 3.456, 2.664, 3.960, 4.032, 4.752, 6.624),
  Maxwi = c(25.416, 31.320, 28.800, 14.544, 29.160, 21.960, 33.624, 28.800, 33.048, 29.376),
  Upsoc = c(23.400, 26.856, 25.344, 28.224, 20.880, 21.600, 28.440, 30.816, 30.456, 32.688),
  Losoc = c(4.608, 6.408, 6.840, 7.848, 3.456, 2.664, 3.960, 4.032, 4.752, 6.624),
  Maxwit = c(45.000, 49.320, 51.120, 53.280, 42.480, 37.440, 53.136, 61.560, 53.784, 55.080),
  Weight = c(4.752, 9.576, 6.912, 7.776, 3.528, 2.376, 8.424, 9.072, 11.664, 7.344))

 Maxle <- "Maxle"
  Socle <- "Socle"
  Maxwi <- "Maxwi"
  Upsoc <- "Upsoc"
  Losoc <- "Losoc"
  Maxwit <- "Maxwit"
  Weight <- "Weight"
```


2.Ajustamos el modelo de regresion lineal para cada una de las variables independientes: 

```{r echo=TRUE}
modelo <- lm(Maxle ~ Socle + Maxwi + Upsoc + Losoc + Maxwit + Weight, data = spear)

 y obtenemos el numero de parametros estimados

num_parametros <- length(coef(modelo))

tambien el calculo del logaritmo de verosimilitud

log_verosimilitud <- logLik(modelo)
```

3.Evaluación de modelos calculando el AIC para cada uno. Esto premia al ajuste del modelo de datos, cuanto menor sea el AIC, mejor es el modelo. El calculo se realiza en base a esta formula AIC=2k−2ln(L^), siendo k el número de parámetros estimados en el modelo y L la función de verosimilitud del modelo, que mide el cómo el modelo se ajusta a los datos.
```{r echo=TRUE}

AIC <- 2 * num_parametros - 2 * log_verosimilitud
```

4.Selección del mejor modelo en base al AIC mas bajo.
```{r echo=TRUE}
print(AIC)
```
El (df=8) proporciona información sobre el ajuste del modelo a los datos observados y la complejidad del modelo, al ser valor 8, este se considera bastante alto, por lo que el modelo tiene una buena capacidad de ajuste a los datos observados.

##15.Defina “modelo anidado”.
Un "modelo anidado" es una relación entre dos modelos estadísticos donde uno de ellos se considera una versión  especial del otro modelo. Un modelo se considera anidado dentro de otro si los parámetros del primer modelo son un subconjunto de los parámetros del segundo modelo y si el primer modelo puede ser obtenido del segundo modelo imponiendo restricciones adicionales en los parámetros.

##16.¿Qué es una iteración?
Una itineracion se refiere a un proceso en el que se realizan secuencias de operaciones o calculos de manera repetitiva con el fin de lograr un resultado deseado.

Extrapolado a la estadistica pordriamos decir que la itineracion es el proceso usado para estimar los parametros de un modelo estadistico, por lo general encontrando soluciones a problemas en los modelos cuando estos no pueden ser resuelto mediante metodos numericos estandar o que estos modelos no proporcionen soluciones analiticas directas. Todo esto a base de calculos o ajustes repetitivos hasta alcanzar esa solucion.

##17.¿Qué mide el error de entrenamiento?
Por lo general el error de entrenamiento mide la discrepancia entre las predicciones del modelo y los valores reales observados en el conjunto de datos de entrenamiento.

##18.¿En qué consiste la estrategia “stepwise”? Implemente dicha estrategia en sus 3 vertientes.
Consiste en identificar un conjunto óptimo de predictores que maximicen la capacidad predictiva del modelo a traves de agregar o eliminar variables del modelo iterativamente, evaluando el impacto en la calidad del ajuste del modelo cada vez que se le realiza algun cambio. 
Esta estrategia posee tres vertientes:
1.Forward stepwise: Comienza con un modelo vacío al que se le agrega una serie de variables a la vez, eligiendo la variable que proporciona incremento en la calidad del ajuste del modelo en cada paso.
```{r echo=TRUE}
modelo <- lm(Maxle ~ Socle + Maxwi + Upsoc + Losoc + Maxwit + Weight, data = spear)

modelo_forward <- stepAIC(modelo, direction = "forward", data = spear)

```
2.Backward stepwise: Comienza con un modelo con todas sus variables y luego se elimina secuencialmente una variable a la vez, eligiendo la variable la cual su eliminacion no repercute en mucha medida la calidad del ajuste del modelo.
```{r echo=TRUE}
modelo_backward <- stepAIC(modelo, direction = "backward", data = spear)
```
3.Stepwise bidireccional: Combina los enfoques de las dos vertientes anteriores, realizando agregaciones o eliminaciones de variables según la mejora en la calidad del ajuste del modelo.
```{r}
modelo_bidireccional <- stepAIC(modelo, direction = "both", data = spear)
```
Y ahora mostramos el resultado de los modelos finales:
```{r echo=TRUE}
print(summary(modelo_forward))
print(summary(modelo_backward))
print(summary(modelo_bidireccional))
```
##19.Evalúe los modelos creados en base al Criterio de Información de Akaike.
Para evaluarlos haremos uso de la funcion AIC:
```{r}
Obtenemos los valores AIC para cada modelo:
  
aic_forward <- AIC(modelo_forward)
aic_backward <- AIC(modelo_backward)
aic_bidireccional <- AIC(modelo_bidireccional)

Y luego vamos a mostrarlos:
print(aic_forward)
print(aic_backward)
print(aic_bidireccional)
```
##20.¿Cuándo ingresa una variable en el modelo? Justifique su respuesta.
##21.Ejecute un modelo con las variables que posean una mayor significancia.
```{r echo=TRUE}
Realizamos un modelo inicial:
  
modelo <- lm(Maxle ~ Socle + Maxwi + Upsoc + Losoc + Maxwit + Weight, data = spear)

Obtenemos los p-value de cada variable:
  p_valores <- summary(modelo)$coef[, "Pr(>|t|)"]

Seleccionamos las variables mas significativas, o sea, las de p-value menos a 0.05:
  variables_significativas <- names(p_valores[p_valores < 0.05])

Y ahora creamos el modelo con las variables significativas:
modelo_subset <- data.frame(Weight = spear$Weight, Maxle = spear$Maxle, Socle= spear$Socle, Maxwi = spear$Maxwi, Upsoc = spear$Upsoc, Losoc = spear$Losoc, Maxwit = spear$Maxwit, spear[, variables_significativas])

modelo_significancia <- lm(Weight ~ Socle + Maxwi + Upsoc + Losoc + Maxwit + Maxle, data = modelo_subset)

Por ultimo usamos el resumen con el resultado final:
  print(summary(modelo_significancia))

```
##22.Calculad la varianza del modelo.
```{r}
varianza <- summary(modelo_significancia)$sigma^2

print(varianza)
```
##23.Calculad los intervalos de confianza de los coeficientes del modelo lineal previamente calculado. Interprete los resultados.
Clculamos los intervalos de confianza con la funcion confint():
```{r echo=TRUE}
intervalos_confianza <- confint(modelo_significancia)
print(intervalos_confianza)
```
##24.Visualice en un grafico de dispersión 3D el modelo lineal.
```{r echo=TRUE}
library(scatterplot3d)

scatterplot3d(modelo_subset$Socle, modelo_subset$Maxwi, modelo_subset$Weight, 
              xlab = "Socle", ylab = "Maxwi", zlab = "Weight",
              main = "Modelo en 3D", color = "magenta")
```
##25.Añada el plano de regresión lineal en un gráfico 3D. Interprete los resultados. (ESTA NO ESTÁ BIEN)
```{r echo=TRUE}
library(scatterplot3d)

Socle <- as.numeric(modelo_subset$Socle)
Maxwi <- as.numeric(modelo_subset$Maxwi)
Weight <- as.numeric(modelo_subset$Weight)

# Ajustar un modelo lineal
modelo_plano_regresion <- lm(Weight ~ Socle + Maxwi, data = modelo_subset)

# Cargar la librería scatterplot3d
library(scatterplot3d)

# Crear el gráfico en 3D
grafico <- scatterplot3d(Socle, Maxwi, Weight, 
              xlab = "Socle", ylab = "Maxwi", zlab = "Weight",
              main = "Modelo en 3D", color = "magenta")

coeficientes <- coef(modelo_plano_regresion)

 plano_regresion <- rgl::planes3d(coeficientes[2], coeficientes[3], -1, coeficientes[1], alpha = 0.5, color = "blue")
 print(plano_regresion)
```
##26. Valide la robustez del modelo lineal. (NI PUTA IDEA TT)
```{r}
1. Análisis de residuos
Grafico de residuos vs valores ajustados con un histograma donde se muestren dichos residuos

plot(modelo, which = 1)

ggplot(data = as.data.frame(residuals(modelo)), aes(x = residuals(modelo))) +
  geom_histogram(binwidth = 1, fill = "magenta", color = "black") +
  ggtitle("Histograma de Residuos") +
  xlab("Residuos") +
  ylab("Frecuencia") +
  theme_minimal()

ggplot(data = as.data.frame(residuals(modelo)), aes(sample = residuals(modelo))) +
  geom_qq() +
  geom_abline(intercept = 0, slope = 1, color = "purple") +
  ggtitle("Plot de Residuos") +
  theme_minimal()

2. Diagnóstico de multicolinealidad: Calculando el VIF.
vif(modelo)

3. Validación cruzada
library(caret)
set.seed(123) # Fijar semilla para reproducibilidad
train_control <- trainControl(method = "cv", number = 5) # Definir control de entrenamiento
modelo_vc <- train(Maxle ~ Socle + Maxwi + Upsoc + Weight, data = spear, method = "lm", trControl = train_control)
print(summary(modelo_vc))

4. Pruebas de hipótesis
summary(modelo)

```
##27.Verifique que se cumplen las condiciones del modelo de regresión lineal múltiple. Justifique su respuesta. 
Para ello mi modelo debe cumplir con los siguientes supuestos:

-   Linealidad: la relación entre las variables independientes y la variable dependiente es lineal.
```{r echo=TRUE}
ggplot(modelo, aes(x = Socle, y = Maxle)) +
  geom_point() +
  labs(x = "Socle", y = "Maxle") +
  ggtitle("Gráfico de dispersión de Socle vs. Maxle")
```

-   Independencia de los residuos: Los residuos del modelo deben ser independientes entre sí. No debe haber patrones discernibles en los residuos cuando se comparan con las variables independientes.
```{r}
residuos_estandarizados <- rstandard(modelo)
plot(seq_along(residuos), residuos_estandarizados, 
     xlab = "Orden de observaciones", ylab = "Residuos estandarizados",
     main = "Independencia de los residuos")
```
-   Homocedasticidad: La variabilidad de los residuos debe ser constante en todos los niveles de las variables independientes.
```{r echo=TRUE}
valores_ajustados <- fitted(modelo)
plot(valores_ajustados, residuos_estandarizados,
     xlab = "Valores ajustados", ylab = "Residuos estandarizados",
     main = "Homocedasticidad de los residuos")
```

-   Normalidad de los residuos: Los residuos del modelo deben seguir una distribución normal. 
```{r echo=TRUE}
qqnorm(residuos, main = "QQ plot de los residuos")
qqline(residuos)
```
Parece ser que nuestro modelo cumple con la condiciones a excepcion de la linealidad, al fin y al cabo nuestras variables en la gráfica presenta una distribucion aleatoria, en cambio, posee homocedasticidad, la distribucion de los residuos es correcta, ya que no posee ningun patron en concreto, y los residuos estan normalizados, porque observamos en el grafico que los puntos siguen prácticamente una linea en diagonal.

##28.¿Emplearía el Factor de Inflación de la Varianza para verificar alguno de las condiciones? Si es así diga cual.
Sí, el Factor de Inflación de la Varianza se emplea para verificar la presencia de multicolinealidad entre los predictores en el modelo de regresión lineal múltiple, por ende, se utiliza para verificar la independencia de los predictores en el modelo. Si el VIF es alto para alguno de los predictores, indica que esa variable está altamente correlacionada con otras variables predictoras, lo que sugiere la presencia de multicolinealidad.
```{r echo=TRUE}
library(car)
vif_modelo <- vif(modelo)

vif_modelo
```
##29.Defina parsimonia.
La parsimonia hace referencia a que el mejor modelo es aquel capaz de explicar con mayor precisión la variabilidad  en la variable respuesta empleando empleando la menor cantidad posible de asunciones.
##30.Verifique la normalidad de los residuos. Dígame la diferencia entre residuos, residuos estandarizados y residuos estudentizados.
```{r}
Verificacion de residuos:
  qqnorm(resid(modelo), col = "magenta")
  qqline(resid(modelo), col = "black")
```
Residuos: Son las diferencias entre los valores observados y los valores predichos por el modelo de regresión, como tal representan el error de predicción del modelo.

Residuos estandarizados: Son los residuos divididos por su desviación estándar, por lo que se usan para evaluar la magnitud de los errores de predicción en relacion a la variabilidad del modelo.

Residuos estudentizados: Son los residuos divididos por su error estándar. Se utilizan para identificar observaciones atípicas o influyentes en el modelo de regresión. 

##31.¿Cómo interpretaría el test de Breusch-Pagan?
La prueba de Breusch-Pagan evalúa si la varianza de los errores es constante o no. 
La interpretacion de este test seria de la siguiente manera:

Si el valor p asociado al test es menor que el nivel de significancia (o.o5)), concluimos que hay evidencia de heterocedasticidad en el modelo.

Si el valor p es mayor, concluimos que no hay suficiente evidencia para afirmar que hay heterocedasticidad.

```{r echo=TRUE}
library(lmtest)
breusch_test <- lmtest::bptest(modelo)
print(breusch_test)
```
##32.Verifique si existen o no observaciones influyentes.
Podemos hacerlo a traves del caculo del leverage, ya que si en este resulta en valores altos, podremos decir que existen valores inflyentes.
```{r echo=TRUE}

leverage <- hatvalues(modelo)
leverage

promedio_leverage <- mean(leverage)
promedio_leverage

observaciones_influyentes <- which(leverage > 2 * promedio_leverage)
observaciones_influyentes

```
Al darnos el resultado named integer (0) esto significa que no existen observaciones influyentes ya que ninguna de ellas supera el doble del promedio de leverage.

##33.Diferencia entre observación atípica y observación que produzca “apalancamiento” del modelo.
-   Observación atípica: Se refiere a un punto de datos que se aleja notablemente del patrón general de los demás datos en el conjunto. Pueden tener impacto en los resultados de los análisis y pueden distorsionar la estimación de los parámetros del modelo. Suelen ser resultado de errores en la medición o en la recopilación de datos.

-   Observación de apalancamiento: Se refiere a una observación que tiene un valor inusualmente alto o bajo en una o más variables predictoras en comparación con las demás observaciones del conjunto de datos. Impactan en la estimación de los coeficientes del modelo y pueden influir en la forma de la línea de regresión. 

##34.Calcule la distancia de Cook. ¿Cómo viene expresada? ¿Cómo la interpretaría?
```{r echo=TRUE}
Calcular los valores predichos y residuos

spear$predichos <- predict(modelo)
spear$residuos <- residuals(modelo)

Calcular la varianza del error
var_error <- deviance(modelo) / modelo$df.residual

Calcular los residuos estandarizados y el apalancamiento
spear$residuos_estandarizados <- rstandard(modelo)
spear$leverage <- hatvalues(modelo)

Calcular la distancia de Cook para cada observación
var_error <- deviance(modelo) / modelo$df.residual
spear$cook <- with(spear, (residuos_estandarizados^2) / (2 * var_error * (1 + leverage)^2))

Mostrar el resultado
print(spear$cook)

```
Los valores más altos de la distancia de Cook, como 0.1688749231 y 0.1146946958, sugieren que las observaciones correspondientes tienen una influencia relativamente alta en los parámetros del modelo. Esto significa que si se eliminan esas observaciones, los coeficientes del modelo pueden cambiar significativamente.
##35.¿Qué valores hat superan la media?
```{r echo=TRUE}
leverage_media <- mean(leverage)

leverage_media_mayor <- leverage[leverage > leverage_media]

leverage_media_mayor
```
##36.¿Como calcularía el límite hat que determina que observaciones son influyentes?
```{r echo=TRUE}
- Número de observaciones
n_observaciones <- nrow(spear)

- Número de predictores en el modelo, incluyendo la constante
predictores <- 6  

- Cálculo del límite HAT
hat_limit <- 2 * (predictores + 1) / n

hat_limit

```

##37.Elimine las observaciones influyentes del modelo y repetir todo el proceso comprobando el peso de esta observación en la regresión lineal. Comente brevemente los resultados.
En el caso de nuestro modelo, ninguno de los valores está por encima de un umbral típicamente utilizado para considerar una observación como influyente (usualmente 4/n, donde n es el número de observaciones). Sin embargo, dado que la observación 1 tiene un valor de Cook de NaN, podríamos considerarla como potencialmente influyente y eliminarla del conjunto de datos.

Voy a eliminar la primera observación y ajustar nuevamente el modelo para ver cómo cambian los resultados.
```{r}
#Eliminar la primera observación
spear_subset <- spear[-1, ]

# Ajustar el modelo nuevamente
modelo_sin_influyentes <- lm(Maxle ~ Socle + Maxwi + Upsoc + Losoc + Maxwit + Weight, data = spear_subset)

# Ver los resultados del nuevo modelo
summary(modelo_sin_influyentes)

```
Después de eliminar la observación que se consideró influyente, observamos varios cambios en los resultados del modelo:

Los coeficientes de regresión para las variables "Socle", "Maxwi", y "Upsoc" mantienen sus signos y magnitudes similares a los del modelo original, aunque el coeficiente de "Socle" ahora es ligeramente menor.

El coeficiente de regresión para la variable "Losoc" no está definido debido a singularidades en los datos después de eliminar la observación influyente. Esto puede deberse a una alta correlación entre "Losoc" y otras variables en el modelo.

Las métricas de ajuste del modelo, como el error estándar residual y el R cuadrado ajustado, muestran cambios. El error estándar residual se ha reducido, lo que sugiere un mejor ajuste del modelo a los datos restantes. Sin embargo, el R cuadrado ajustado también se ha reducido, lo que indica que el modelo explica una menor proporción de la variabilidad en los datos después de eliminar la observación influyente.

En resumen, la eliminación de la observación influyente ha afectado los resultados del modelo, aunque algunos coeficientes y métricas de ajuste permanecen relativamente similares.